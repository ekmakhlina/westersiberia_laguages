{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "forest-enets texts_from_ siberianlanguages_to_json.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "q92JruuN-K03"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import collections\n",
        "from nltk import word_tokenize, wordpunct_tokenize\n",
        "import itertools\n",
        "import nltk\n",
        "import numpy as np\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "\n",
        "#nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WMXWy_lUP21"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "YourURL = 'http://www.siberianlanguages.surrey.ac.uk/audio/a-calm-year/'\n",
        "r = requests.get(YourURL)\n",
        "\n",
        "soup = BeautifulSoup(r.content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dn7IJwVIEuYO"
      },
      "source": [
        "def get_all_content_parts(soup):\n",
        "  all_content = soup.find_all('div', {\"class\": \"tier\"}) \n",
        "  all_content_parts = []\n",
        "  part = []\n",
        "  for c in all_content:\n",
        "    if c.attrs['data-tier']=='ts_content_forestenets':\n",
        "      all_content_parts.append(part)\n",
        "      part = []\n",
        "      part.append(c)\n",
        "    else:\n",
        "      part.append(c)\n",
        "  all_content_parts.append(part)\n",
        "  print(all_content_parts)\n",
        "  return all_content_parts[1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkIN1reucnS8"
      },
      "source": [
        "def make_all_sentence_df(all_content_parts):\n",
        "  index = [i for i in range(len(all_content_parts))]\n",
        "  result = pd.DataFrame(index=index, columns=['sentence','morphemes','glosses_en', 'glosses_ru', 'translation_en', 'translation_ru'])\n",
        "  for i in index:\n",
        "    for c in all_content_parts[i]:\n",
        "      if c.attrs['data-tier']=='ts_content_forestenets':\n",
        "        result['sentence'][i] = c.string\n",
        "      if c.attrs['data-tier']=='ts_content_morph':\n",
        "        result['morphemes'][i] = c.string\n",
        "      if c.attrs['data-tier']=='ts_content_igt':\n",
        "        result['glosses_en'][i] = c.string \n",
        "      if c.attrs['data-tier']=='ts_content_igt_ru':\n",
        "        result['glosses_ru'][i] = c.string    \n",
        "      if c.attrs['data-tier']=='ts_content_eng':\n",
        "        result['translation_en'][i] = c.string\n",
        "      if c.attrs['data-tier']=='ts_content_rus':\n",
        "        result['translation_ru'][i] = c.string\n",
        "  print(result)\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3ffOntbIzgF"
      },
      "source": [
        "def string_into_tokens(s):\n",
        "    wo_brackets = [i for i in s.split() if '[' not in s]\n",
        "    ll = [[word_tokenize(w), ' '] for w in wo_brackets]\n",
        "    s_list = list(itertools.chain(*list(itertools.chain(*ll))))\n",
        "    return s_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVAmGm1Ndctb"
      },
      "source": [
        "def make_sentence_words_arr(all_sentences, i):\n",
        "    #print(i)\n",
        "    wf_arr = []\n",
        "    tokens = string_into_tokens(all_sentences['sentence'][i])\n",
        "    #print(tokens)\n",
        "    #print(all_sentences['morphemes'][i])\n",
        "    try:\n",
        "      morphemes = all_sentences['morphemes'][i].split()\n",
        "    except AttributeError:\n",
        "      morphemes = np.nan\n",
        "    #print(morphemes)\n",
        "    #print(len(morphemes))\n",
        "    try:\n",
        "      glosses_en = all_sentences['glosses_en'][i].split()\n",
        "    except AttributeError:\n",
        "      glosses_en = np.nan\n",
        "    try:\n",
        "      glosses_ru = all_sentences['glosses_ru'][i].split()\n",
        "    except:\n",
        "      glosses_ru = np.nan\n",
        "    j = 0\n",
        "    off_start = 0\n",
        "    sentence_index = 0\n",
        "    for k in range(len(tokens)):\n",
        "        if tokens[k] == ' ':\n",
        "            off_start += len(tokens[k])\n",
        "            continue\n",
        "        elif tokens[k] in string.punctuation:\n",
        "            wf_arr.append(collections.OrderedDict({\n",
        "                'wf': tokens[k],\n",
        "                'wtype': 'punct',\n",
        "                'off_start': off_start,\n",
        "                'off_end': off_start + len(tokens[k]),\n",
        "                'next_word': sentence_index+1,\n",
        "                'sentence_index': sentence_index\n",
        "                #'ana': False\n",
        "            }))\n",
        "            sentence_index += 1\n",
        "        else:\n",
        "            try:\n",
        "              glosses_en_j=glosses_en[j]\n",
        "            except:\n",
        "              glosses_en_j=''  \n",
        "            try:\n",
        "              glosses_ru_j=glosses_ru[j]\n",
        "            except:\n",
        "              glosses_ru_j=''\n",
        "            try:\n",
        "              morphemes_j=morphemes[j]\n",
        "            except:\n",
        "              morphemes_j=''            \n",
        "            wf_arr.append(collections.OrderedDict({\n",
        "                'wf': tokens[k],\n",
        "                'wtype': 'word',\n",
        "                'off_start': off_start,\n",
        "                'off_end': off_start + len(tokens[k]),\n",
        "                'next_word': sentence_index+1,\n",
        "                'sentence_index': sentence_index,\n",
        "                'ana': [{'glosses_ru': glosses_ru_j, 'glosses_en': glosses_en_j, 'morphemes': morphemes_j}]\n",
        "                #\"ana\": [{\"lex\": \"ojorsp\", \"gr.pos\": \"V\", \"gr.pers\": \"2\", \"gr.tense\": \"t1\"}]\n",
        "            }))\n",
        "            j += 1\n",
        "            sentence_index += 1\n",
        "        off_start += len(tokens[k])\n",
        "    return wf_arr\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qKLzuJqY0Mh"
      },
      "source": [
        "def make_json(soup):\n",
        "  all_sentences = []\n",
        "  all_sentences_df = make_all_sentence_df(get_all_content_parts(soup))\n",
        "  for i in range(len(all_sentences_df)):\n",
        "    all_sentences.append(collections.OrderedDict({\n",
        "              'text': all_sentences_df['sentence'][i],\n",
        "              'words': make_sentence_words_arr(all_sentences_df, i),\n",
        "              'lang': 1\n",
        "          }))\n",
        "  one_text = {\n",
        "    \"sentences\": all_sentences,\n",
        "    \"meta\": {\"author\": 'author', \"title\": soup.title.string, \"year\": '', \"id\": 1}\n",
        "  }\n",
        "  return one_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLxBXEr29o1X"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpjC3hhKD_3K"
      },
      "source": [
        "def get_all_urls(main_url):\n",
        "  all_urls = []\n",
        "  r = requests.get(main_url)\n",
        "  soup = BeautifulSoup(r.content)\n",
        "  titles = soup.find_all('h1', {'class': \"entry-title\"})\n",
        "  for title in titles:\n",
        "    all_urls.append(next(title.children).attrs['href'])\n",
        "  return all_urls\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bb3PNw4aGXws"
      },
      "source": [
        "!mkdir 'forest-enets'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v52Q-DIx_k8c"
      },
      "source": [
        "main_url = 'http://www.siberianlanguages.surrey.ac.uk/language/forest-enets/?post_types=ava,avv'\n",
        "all_urls = get_all_urls(main_url)\n",
        "for url in all_urls:\n",
        "  r = requests.get(url)\n",
        "  soup = BeautifulSoup(r.content)\n",
        "  text_json = make_json(soup)\n",
        "  name = 'forest-enets/' + soup.title.string + '.json'\n",
        "  with open(name, 'w', encoding='utf-8') as f:\n",
        "      json.dump(text_json, f, ensure_ascii=False, indent=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CpLlQX1HrR9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd5cc6b0-bc71-4274-ce74-26ab9ba78290"
      },
      "source": [
        "print('ura2')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ura2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dld1XE-9GaWT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab79bd54-ed1b-4284-9562-745c9b25a45d"
      },
      "source": [
        "!zip -r forest-enets.zip forest-enets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: forest-enets/ (stored 0%)\n",
            "  adding: forest-enets/A clairvoyant - Endangered Languages and Cultures of Siberia.json (deflated 95%)\n",
            "  adding: forest-enets/The one legged woman - Endangered Languages and Cultures of Siberia.json (deflated 95%)\n",
            "  adding: forest-enets/A calm year - Endangered Languages and Cultures of Siberia.json (deflated 94%)\n",
            "  adding: forest-enets/Little red riding hood - Endangered Languages and Cultures of Siberia.json (deflated 94%)\n",
            "  adding: forest-enets/The small bear cub - Endangered Languages and Cultures of Siberia.json (deflated 93%)\n",
            "  adding: forest-enets/Duck's nest - Endangered Languages and Cultures of Siberia.json (deflated 93%)\n",
            "  adding: forest-enets/Fish and chitchat (5) - Endangered Languages and Cultures of Siberia.json (deflated 94%)\n",
            "  adding: forest-enets/The bears, the food sledge and the helicopter - Endangered Languages and Cultures of Siberia.json (deflated 94%)\n",
            "  adding: forest-enets/Fish and chitchat (2) - Endangered Languages and Cultures of Siberia.json (deflated 94%)\n",
            "  adding: forest-enets/Cooking fish - Endangered Languages and Cultures of Siberia.json (deflated 94%)\n",
            "  adding: forest-enets/My forefather and his bear cubs - Endangered Languages and Cultures of Siberia.json (deflated 95%)\n",
            "  adding: forest-enets/A poor man uses dogs - Endangered Languages and Cultures of Siberia.json (deflated 93%)\n",
            "  adding: forest-enets/Dogs in the Tundra - Endangered Languages and Cultures of Siberia.json (deflated 92%)\n",
            "  adding: forest-enets/Bait - Endangered Languages and Cultures of Siberia.json (deflated 92%)\n",
            "  adding: forest-enets/A contemporary chat - Endangered Languages and Cultures of Siberia.json (deflated 94%)\n",
            "  adding: forest-enets/The man who wanted to speak - Endangered Languages and Cultures of Siberia.json (deflated 94%)\n",
            "  adding: forest-enets/The old man and the living pike - Endangered Languages and Cultures of Siberia.json (deflated 95%)\n",
            "  adding: forest-enets/A stone with a hole - Endangered Languages and Cultures of Siberia.json (deflated 94%)\n",
            "  adding: forest-enets/Going to school on a reindeer - Endangered Languages and Cultures of Siberia.json (deflated 94%)\n",
            "  adding: forest-enets/Fish and chitchat - Endangered Languages and Cultures of Siberia.json (deflated 95%)\n",
            "  adding: forest-enets/The freak and his brother - Endangered Languages and Cultures of Siberia.json (deflated 94%)\n",
            "  adding: forest-enets/Fish and chitchat (4) - Endangered Languages and Cultures of Siberia.json (deflated 95%)\n",
            "  adding: forest-enets/A little chat from the 60's - Endangered Languages and Cultures of Siberia.json (deflated 95%)\n",
            "  adding: forest-enets/Traces - Endangered Languages and Cultures of Siberia.json (deflated 94%)\n",
            "  adding: forest-enets/Enets clans - Endangered Languages and Cultures of Siberia.json (deflated 94%)\n",
            "  adding: forest-enets/The black duck - Endangered Languages and Cultures of Siberia.json (deflated 92%)\n",
            "  adding: forest-enets/A clever dog - Endangered Languages and Cultures of Siberia.json (deflated 94%)\n",
            "  adding: forest-enets/The dogs' plague - Endangered Languages and Cultures of Siberia.json (deflated 94%)\n",
            "  adding: forest-enets/Wild reindeer - Endangered Languages and Cultures of Siberia.json (deflated 95%)\n",
            "  adding: forest-enets/A contemporary chat (2) - Endangered Languages and Cultures of Siberia.json (deflated 94%)\n",
            "  adding: forest-enets/Lost meat - Endangered Languages and Cultures of Siberia.json (deflated 95%)\n",
            "  adding: forest-enets/A stone with a hole 2 - Endangered Languages and Cultures of Siberia.json (deflated 94%)\n",
            "  adding: forest-enets/Chat with a girl - Endangered Languages and Cultures of Siberia.json (deflated 94%)\n",
            "  adding: forest-enets/The cloudberries - Endangered Languages and Cultures of Siberia.json (deflated 93%)\n",
            "  adding: forest-enets/A man and the one-legged woman - Endangered Languages and Cultures of Siberia.json (deflated 94%)\n",
            "  adding: forest-enets/The Evenki man - Endangered Languages and Cultures of Siberia.json (deflated 95%)\n",
            "  adding: forest-enets/Duck's eggs - Endangered Languages and Cultures of Siberia.json (deflated 93%)\n",
            "  adding: forest-enets/Fish and chitchat (3) - Endangered Languages and Cultures of Siberia.json (deflated 95%)\n",
            "  adding: forest-enets/Cooking meat - Endangered Languages and Cultures of Siberia.json (deflated 94%)\n",
            "  adding: forest-enets/Chaga tea - Endangered Languages and Cultures of Siberia.json (deflated 93%)\n",
            "  adding: forest-enets/Djoa - Endangered Languages and Cultures of Siberia.json (deflated 95%)\n",
            "  adding: forest-enets/Bear fat - Endangered Languages and Cultures of Siberia.json (deflated 94%)\n",
            "  adding: forest-enets/There are no shamans here! - Endangered Languages and Cultures of Siberia.json (deflated 94%)\n",
            "  adding: forest-enets/A contemporary chat (3) - Endangered Languages and Cultures of Siberia.json (deflated 94%)\n",
            "  adding: forest-enets/Beware of the gulls! - Endangered Languages and Cultures of Siberia.json (deflated 94%)\n",
            "  adding: forest-enets/The two mates - Endangered Languages and Cultures of Siberia.json (deflated 95%)\n",
            "  adding: forest-enets/Moving camps in the past - Endangered Languages and Cultures of Siberia.json (deflated 94%)\n",
            "  adding: forest-enets/Stalin's prisoner - Endangered Languages and Cultures of Siberia.json (deflated 94%)\n",
            "  adding: forest-enets/Berry jam recipe - Endangered Languages and Cultures of Siberia.json (deflated 94%)\n",
            "  adding: forest-enets/Changes on the lake - Endangered Languages and Cultures of Siberia.json (deflated 93%)\n",
            "  adding: forest-enets/Ducks - Endangered Languages and Cultures of Siberia.json (deflated 93%)\n",
            "  adding: forest-enets/Bears and fishing nets - Endangered Languages and Cultures of Siberia.json (deflated 93%)\n",
            "  adding: forest-enets/Pregnancy superstitions - Endangered Languages and Cultures of Siberia.json (deflated 94%)\n",
            "  adding: forest-enets/Ducks move their eggs - Endangered Languages and Cultures of Siberia.json (deflated 93%)\n",
            "  adding: forest-enets/Two men - Endangered Languages and Cultures of Siberia.json (deflated 95%)\n",
            "  adding: forest-enets/The light reindeer from the north - Endangered Languages and Cultures of Siberia.json (deflated 93%)\n",
            "  adding: forest-enets/A forest enets dialog - Endangered Languages and Cultures of Siberia.json (deflated 94%)\n",
            "  adding: forest-enets/Hunting wild reindeer - Endangered Languages and Cultures of Siberia.json (deflated 94%)\n",
            "  adding: forest-enets/A little chat from the 60's (2) - Endangered Languages and Cultures of Siberia.json (deflated 94%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_GSenNiIQUr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}